knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("graph/graph1.jpeg")
library(tidymodels)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(corrplot)
library(vip)
library(janitor)
library(kknn)
library(dplyr)
mydata <- read.csv("data/unprocessed/131dataset.csv")
head(mydata)
mydata <- subset(mydata, select = -c(X,SEASON,FGM,FGA,X3PM,X3PA,FTM,FTA))
plusminus<-mydata[,23]
mydata <- mydata[,-c(23)]
mydata$plusminus <- plusminus
mydata <- subset(mydata, select = -c(FP))
mydata <- as_tibble(mydata) %>%
clean_names()
dim(mydata)
mydata$team <- as.factor(mydata$team)
write.csv(mydata,"data/processed/cleaneddata.csv",row.names=FALSE)
sum(is.na(mydata))
mydata %>%
ggplot(aes(rankings)) +
geom_bar() +
labs(
title = "Distribution of ratings"
)
mydata %>%
ggplot(aes(pts)) +
geom_bar() +
labs(
title = "distribution of points"
)
mydata %>%
select(is.numeric) %>%
cor() %>%
corrplot()
mydata <- subset(mydata, select = -c(pf,tov))
mydata %>%
select(is.numeric) %>%
cor() %>%
corrplot()
mydata_split <- initial_split(mydata, prop = 0.8,
strata = rankings)
mydata_train <- training(mydata_split)
mydata_test <- testing(mydata_split)
mydata_folds <- vfold_cv(mydata_train, v = 5, strata = rankings)
nrow(mydata_train)/nrow(mydata_test)
my_recipe <- recipe(rankings ~ gp+w+l+min+pts+fg+x3p+ft
+oreb+dreb+ast+stl+blk+dd2+td3+plusminus, data = mydata_train) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rec_poly <- recipe(rankings ~ gp+w+l+min+pts+fg+x3p+ft
+oreb+dreb+ast+stl+blk+dd2+td3+plusminus, data = mydata_train) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_poly(pts, degree = tune())
rec_tree <- recipe(rankings ~ gp+w+l+min+pts+fg+x3p+ft
+oreb+dreb+ast+stl+blk+dd2+td3+plusminus, data = mydata_train) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_normalize(all_predictors())
lm_mod <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
lm_wkflow <- workflow() %>%
add_model(lm_mod) %>%
add_recipe(my_recipe)
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_mode("regression") %>%
set_engine("kknn")
knn_wkflow <- workflow() %>%
add_model(knn_mod) %>%
add_recipe(my_recipe)
en_mod <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_mode("regression") %>%
set_engine("glmnet")
en_wkflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(en_mod)
poly_wkflow <- workflow() %>%
add_model(lm_mod) %>%
add_recipe(rec_poly)
tree_spec <- decision_tree(cost_complexity = tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
tree_wkflow <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(rec_tree)
knn_grid <- grid_regular(neighbors(range = c(1, 10)), levels = 10)
en_grid <- grid_regular(penalty(range =),
mixture(range = c(0, 1)),
levels = 10)
poly_grid <- grid_regular(degree(range = c(1, 4)),
levels = 4)
tree_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
knn_tune <- tune_grid(
knn_wkflow,
resamples = mydata_folds,
grid = knn_grid
)
en_tune <- tune_grid(
en_wkflow,
resamples = mydata_folds,
grid = en_grid
)
lm_fit <- lm_wkflow %>%
fit_resamples(resamples = mydata_folds)
poly_tune <- tune_grid(
poly_wkflow,
resamples = mydata_folds,
grid = poly_grid
)
tree_tune <- tune_grid(
tree_wkflow,
resamples = mydata_folds,
grid = tree_grid
)
lm_rmse <- collect_metrics(lm_fit) %>%
slice(1)
best_knn <- collect_metrics(knn_tune) %>%
arrange(mean) %>%
slice(11)
best_en <- collect_metrics(en_tune) %>%
arrange(mean) %>%
slice(101)
best_poly <-  poly_tune %>%
collect_metrics() %>%
arrange(mean) %>%
slice(5)
best_tree <- tree_tune %>%
collect_metrics() %>%
arrange(mean) %>%
slice(11)
models_rmse <- data.frame(models = c("Linear Regression", "Poly Regression", "KNN Regression", "Elastic Net Regression", "Decision Trees"),
rmse = c(lm_rmse$mean, best_poly$mean, best_knn$mean, best_en$mean, best_tree$mean))
models_rmse
autoplot(tree_tune, metric = 'rmse')
autoplot(poly_tune, metric = 'rmse')
autoplot(knn_tune, metric = 'rmse')
best_poly
best_poly_train <- select_best(poly_tune, metric = 'rmse')
poly_final_wkflow_train <- finalize_workflow(poly_wkflow, best_poly_train)
poly_fit_train <- fit(poly_final_wkflow_train,data=mydata_train)
myprediction <- predict(poly_fit_train,new_data = mydata_test)
myprediction <- bind_cols(myprediction,select(mydata_test,rankings))
myprediction
mymetric <- metric_set(rmse)
prediction_metric <- mymetric(myprediction,truth=rankings, estimate=.pred)
prediction_metric
myprediction %>%
ggplot(aes(x = .pred, y = rankings)) +
geom_point(alpha=0.4,color='red') +
geom_abline(lty = 3) +
coord_obs_pred() +
labs(title = "Predicted Values versus Actual Values")
poly_fit_train %>% extract_fit_parsnip() %>%
vip() +
theme_minimal()+
labs(title = "Variable Importance")
knitr::opts_chunk$set(echo = TRUE)
knitr::include_graphics("graph/graph1.jpeg")
library(tidymodels)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(corrplot)
library(vip)
library(janitor)
library(kknn)
library(dplyr)
mydata <- read.csv("data/unprocessed/131dataset.csv")
head(mydata)
mydata <- subset(mydata, select = -c(X,SEASON,FGM,FGA,X3PM,X3PA,FTM,FTA))
plusminus<-mydata[,23]
mydata <- mydata[,-c(23)]
mydata$plusminus <- plusminus
mydata <- subset(mydata, select = -c(FP))
mydata <- as_tibble(mydata) %>%
clean_names()
dim(mydata)
mydata$team <- as.factor(mydata$team)
write.csv(mydata,"data/processed/cleaneddata.csv",row.names=FALSE)
sum(is.na(mydata))
mydata %>%
ggplot(aes(rankings)) +
geom_bar() +
labs(
title = "Distribution of ratings"
)
mydata %>%
ggplot(aes(pts)) +
geom_bar() +
labs(
title = "distribution of points"
)
mydata %>%
select(is.numeric) %>%
cor() %>%
corrplot()
mydata <- subset(mydata, select = -c(pf,tov))
mydata %>%
select(is.numeric) %>%
cor() %>%
corrplot()
set.seed(1)
mydata_split <- initial_split(mydata, prop = 0.8,
strata = rankings)
mydata_train <- training(mydata_split)
mydata_test <- testing(mydata_split)
mydata_folds <- vfold_cv(mydata_train, v = 5, strata = rankings)
nrow(mydata_train)/nrow(mydata_test)
my_recipe <- recipe(rankings ~ gp+w+l+min+pts+fg+x3p+ft
+oreb+dreb+ast+stl+blk+dd2+td3+plusminus, data = mydata_train) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
rec_poly <- recipe(rankings ~ gp+w+l+min+pts+fg+x3p+ft
+oreb+dreb+ast+stl+blk+dd2+td3+plusminus, data = mydata_train) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_poly(pts, degree = tune())
rec_tree <- recipe(rankings ~ gp+w+l+min+pts+fg+x3p+ft
+oreb+dreb+ast+stl+blk+dd2+td3+plusminus, data = mydata_train) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_normalize(all_predictors())
lm_mod <- linear_reg() %>%
set_mode("regression") %>%
set_engine("lm")
lm_wkflow <- workflow() %>%
add_model(lm_mod) %>%
add_recipe(my_recipe)
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_mode("regression") %>%
set_engine("kknn")
knn_wkflow <- workflow() %>%
add_model(knn_mod) %>%
add_recipe(my_recipe)
en_mod <- linear_reg(penalty = tune(),
mixture = tune()) %>%
set_mode("regression") %>%
set_engine("glmnet")
en_wkflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(en_mod)
poly_wkflow <- workflow() %>%
add_model(lm_mod) %>%
add_recipe(rec_poly)
tree_spec <- decision_tree(cost_complexity = tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
tree_wkflow <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(rec_tree)
knn_grid <- grid_regular(neighbors(range = c(1, 10)), levels = 10)
en_grid <- grid_regular(penalty(range =),
mixture(range = c(0, 1)),
levels = 10)
poly_grid <- grid_regular(degree(range = c(1, 4)),
levels = 4)
tree_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)
knn_tune <- tune_grid(
knn_wkflow,
resamples = mydata_folds,
grid = knn_grid
)
en_tune <- tune_grid(
en_wkflow,
resamples = mydata_folds,
grid = en_grid
)
lm_fit <- lm_wkflow %>%
fit_resamples(resamples = mydata_folds)
poly_tune <- tune_grid(
poly_wkflow,
resamples = mydata_folds,
grid = poly_grid
)
tree_tune <- tune_grid(
tree_wkflow,
resamples = mydata_folds,
grid = tree_grid
)
lm_rmse <- collect_metrics(lm_fit) %>%
slice(1)
best_knn <- collect_metrics(knn_tune) %>%
arrange(mean) %>%
slice(11)
best_en <- collect_metrics(en_tune) %>%
arrange(mean) %>%
slice(101)
best_poly <-  poly_tune %>%
collect_metrics() %>%
arrange(mean) %>%
slice(5)
best_tree <- tree_tune %>%
collect_metrics() %>%
arrange(mean) %>%
slice(11)
models_rmse <- data.frame(models = c("Linear Regression", "Poly Regression", "KNN Regression", "Elastic Net Regression", "Decision Trees"),
rmse = c(lm_rmse$mean, best_poly$mean, best_knn$mean, best_en$mean, best_tree$mean))
models_rmse
autoplot(tree_tune, metric = 'rmse')
autoplot(poly_tune, metric = 'rmse')
autoplot(knn_tune, metric = 'rmse')
best_poly
best_poly_train <- select_best(poly_tune, metric = 'rmse')
poly_final_wkflow_train <- finalize_workflow(poly_wkflow, best_poly_train)
poly_fit_train <- fit(poly_final_wkflow_train,data=mydata_train)
myprediction <- predict(poly_fit_train,new_data = mydata_test)
myprediction <- bind_cols(myprediction,select(mydata_test,rankings))
myprediction
mymetric <- metric_set(rmse)
prediction_metric <- mymetric(myprediction,truth=rankings, estimate=.pred)
prediction_metric
myprediction %>%
ggplot(aes(x = .pred, y = rankings)) +
geom_point(alpha=0.4,color='red') +
geom_abline(lty = 3) +
coord_obs_pred() +
labs(title = "Predicted Values versus Actual Values")
poly_fit_train %>% extract_fit_parsnip() %>%
vip() +
theme_minimal()+
labs(title = "Variable Importance")
